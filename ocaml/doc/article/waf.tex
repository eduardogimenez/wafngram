\documentclass[runningheads,a4paper]{llncs}

\usepackage[american]{babel}
\usepackage[american]{amsmath}

%better font, similar to the default springer font
%cfr-lm is preferred over lmodern. Reasoning at http://tex.stackexchange.com/a/247543/9075
\usepackage[%
rm={oldstyle=false,proportional=true},%
sf={oldstyle=false,proportional=true},%
tt={oldstyle=false,proportional=true,variable=true},%
qt=false%
]{cfr-lm}
%
%if more space is needed, exchange cfr-lm by mathptmx
%\usepackage{mathptmx}

\usepackage{graphicx}

%extended enumerate, such as \begin{compactenum}
\usepackage{paralist}

%put figures inside a text
%\usepackage{picins}
%use
%\piccaptioninside
%\piccaption{...}
%\parpic[r]{\includegraphics ...}
%Text...

%Sorts the citations in the brackets
%\usepackage{cite}

\usepackage[T1]{fontenc}

%for demonstration purposes only
\usepackage[math]{blindtext}

%for easy quotations: \enquote{text}
\usepackage{csquotes}

%enable margin kerning
\usepackage{microtype}

%tweak \url{...}
\usepackage{url}
%nicer // - solution by http://tex.stackexchange.com/a/98470/9075
\makeatletter
\def\Url@twoslashes{\mathchar`\/\@ifnextchar/{\kern-.2em}{}}
\g@addto@macro\UrlSpecials{\do\/{\Url@twoslashes}}
\makeatother
\urlstyle{same}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother

%diagonal lines in a table - http://tex.stackexchange.com/questions/17745/diagonal-lines-in-table-cell
%slashbox is not available in texlive (due to licensing) and also gives bad results. This, we use diagbox
%\usepackage{diagbox}

%required for pdfcomment later
\usepackage{xcolor}

% new packages BEFORE hyperref
% See also http://tex.stackexchange.com/questions/1863/which-packages-should-be-loaded-after-hyperref-instead-of-before

%enable hyperref without colors and without bookmarks
\usepackage[
%pdfauthor={},
%pdfsubject={},
%pdftitle={},
%pdfkeywords={},
bookmarks=false,
breaklinks=true,
colorlinks=true,
linkcolor=black,
citecolor=black,
urlcolor=black,
%pdfstartpage=19,
pdfpagelayout=SinglePage,
pdfstartview=Fit
]{hyperref}
%enables correct jumping to figures when referencing
\usepackage[all]{hypcap}

%enable nice comments
\usepackage{pdfcomment}
\newcommand{\commentontext}[2]{\colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{\pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

%compatibality with TODO package
\newcommand{\todo}[1]{\commentatside{#1}}

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link
\usepackage[capitalise,nameinlink]{cleveref}
%Nice formats for \cref
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}

\usepackage{xspace}
%\newcommand{\eg}{e.\,g.\xspace}
%\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\eg}{e.\,g.,\ }
\newcommand{\ie}{i.\,e.,\ }

%introduce \powerset - hint by http://matheplanet.com/matheplanet/nuke/html/viewtopic.php?topic=136492&post_id=997377
\DeclareFontFamily{U}{MnSymbolC}{}
\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
\DeclareFontShape{U}{MnSymbolC}{m}{n}{
    <-6>  MnSymbolC5
   <6-7>  MnSymbolC6
   <7-8>  MnSymbolC7
   <8-9>  MnSymbolC8
   <9-10> MnSymbolC9
  <10-12> MnSymbolC10
  <12->   MnSymbolC12%
}{}
\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
%This allows a copy'n'paste of the text from the paper
\input glyphtounicode.tex
\pdfgentounicode=1

\title{Detection of Web Applications through Language Recognition}
%If Title is too long, use \titlerunning
%\titlerunning{Short Title}

%Single insitute
\author{Eduardo Giménez}
%If there are too many authors, use \authorrunning
%\authorrunning{First Author et al.}
\institute{ICT4V}

%Multiple insitutes
%Currently disabled
%
\iffalse
%Multiple institutes are typeset as follows:
\author{Firstname Lastname\inst{1} \and Firstname Lastname\inst{2} }
%If there are too many authors, use \authorrunning
%\authorrunning{First Author et al.}

\institute{
Insitute 1\\
\email{...}\and
Insitute 2\\
\email{...}
}
\fi
			
\maketitle

\begin{abstract}
The abstract.
\end{abstract}

\keywords{...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Context and motivations}\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This work focuses on preventing the misuse of Web Applications. A web application is piece of software that develops a coordinated set of functions based on a client-server arquitecture, in which the client runs on the user’s web browser. The information between the user’s client and the application server is transmitted using the HTTP protocol.

The kind of applications in which we are mainly interested are the ones used for e-government purposes. These are rather simple web applications which propose register and consulting functions, mostly made of static HTML. They either displays information to the user, lets the user to fill in a form to register some piece of information, or in order to return some piece of information to the user, such as an HTML page including information in a database, a PDF file or the like. 

By its very nature, web applications are designed to be exposed in the Internet. This means that its owner can not govern which user actually connects to the application and  therefore are available to any Internet user. This feature makes them a primary target of any attacker that wants to get access to the assets (data bases) that the application manages or to place baits (e.g., fake URLs to his own site) to lure honest users.


The code of a web application may contain vulnerabilities, and it is usually unknown which they are or where are they placed. For instance, a field in some form could be sensible to SQL injection, the database could have been previously infected with an URL that is displayed in the returned HTML page, or any of the vulnerabilities described in [OWASP]. Placed in this context, we are interested in developing automated tools that could help us to prevent the attackers from exploiting those vulnerabilities, or once exploited, can be easily deployed to content the incident minimizing the damages. 

Furthermore, our main concern is not fixing the underlying problem, namely, the buggy web application code which contains the vulberabilities. The reason is that searching for those bugs is a time consuming activity and frequently the application can not remain off-line until it has been successfully completed. For the very nature of the applications we are interested in, downtime has to be minimnized as much as possible. Moreover, in some cases the source code of the web application may not be easily available for inspection, either because the provider who sold it to the organization is reluctant to show the details of its code, or because the organization has no longer trace of the source code itself.

Instead of focusing in the web application code, we are rather interested in virtual patching. Virtual patching is a technique in which a second software component, external to the web application, is placed between the web application and its users. This component, usually called a Web Application Firewall (WAF), intercepts and inspects all the traffic between the server and the clients, searching for those packets that look like part of an attack. Once recognized, the suspicious packets are then processed in a different way (logged, suppressed, derived to a honeypot application or whatever). ModSecurity [ModSec] is an open source, widely used WAF enabling real-time web application monitoring, logging, and access control. It gives access to the HTTP traffic stream, in real-time, along with the ability to inspect it and take any of the aforementioned actions. 
The actions that ModSecurity undertakes are driven by rules. The administrator specifies rules about the contents of the HTTP packets through regular expressions. ModSecurity intercepts each packet from and to the protected web application. If the packet matches a rule, then the actions specified in that rules are fired. ModSecurity offers a default package of rules for tackling the most usual vulnerabilities spotted in the OWASP Top 10, known as the OWASP ModSecurity Core Rule Set (CRS). 

The possibility for the user to specify its own rules rather than using a wired policy is one of the interesting features of ModSecurity, which enables to tailor the WAF to each specific application. However, an approach only based on rules also has some drawbacks. First, rules are a static and rigid by nature. They are well suited for describing known attack scenarios, but they may fail to cover slightly different ones, or even totally new ones. Also, as a side effect, covering all the attack variants usually result in quite complicated regular expressions and rules, which are difficult to understand, maintain and modify. Second, practice has shown that the CRS usually produce false positives. Such situations are handled by tuning the rules through the introduction of exceptions. Exception rules handle specific situations for specific fields or cases which require to allow traffic matching some of the CRS rules. However, this is usually a time consuming and error prone task, which has to be done for each specific web application.

In tradicional networks firewalls and IDS, the approach based on rules has been successfully complemented with other tools based on machine learning, anomaly detection and other statistical approaches which provides higher levels of flexibility and adaptability. Those approaches take advantage of sample data about what the “normal” behavior of the web application is, in order to spot suspicious situations which fall out of this “normal use” (anomalies), and which could correspond to on-going attacks. Our long term purpose is to improve ModSecurity with such anomaly detection techniques.

\section{Code injection attacks}

A usual use case in e-governement web applications is to ask the user to fill a form wiht data, which is then used to assemble an SQL database query and return the retrieved information to the user. In many programming languages, the programmer assembles the query concatenating strings, including part of the data supplied by the user in the form. If such data is not correctly sanitized, an attacker may take advantage of this mthod to inject a carefully crafted piece of code that discloses unauthorized information or internally damages the application. For example, the form contains a field \textit{user} that the application uses to assemble the following query as the string \texttt{query := "SELECT * FROM users WHERE name = '" + }\textit{userName}\texttt{ + “ ';"}. If the field is filled with the data \texttt{John Smith}, then the record for John Smith is the users table of the database is returned and displayed. However, should the attacker enter the data \texttt{John Smith’; DROP TABLE users; SELECT userinfo WHERE ‘t’=‘t}, the string results in the query \texttt{query := ``SELECT * FROM users WHERE name = ‘John Smith’;DROP TABLE users;SELECT userinfo  WHERE ‘t’=‘t’}, which removes the users table from the application database. 

In its most general version, code injection consists in the malicious provision of unexpected input data which modifies the intended purpose of an executable, interpreted piece of code. Beyond SQL, this technique may be applied to many query languages that are usually used in web applications, such as XLDAP, OS commands, XPath, XML parsers, SMTP headers, NoSQL, among others. It may be applied no only to human supplied form fields, but also to other fields in the HTTP requests that usually are automatically filled by the web navigator, and which are used on the web server side to perform queries about supported features, better rendering, etc. 

\section{A language perspective}

From a language perspective, code injection attacks always involve mixing two languages: the expected language for a given field and a piece of a query programming language. In the former example, the expected language for the field correspond to personal names in natrual language, while the query programming language is SQL. Starting from this fact, we propose that detecting such kind of attacks may be considered as a particular case of detecting multiple languages in a given piece text. Our research program is therefore to find conexions between NLP techniques and attack detection in WAF, through the application of pattern recognition techniques.

A well known technique for identifying language of a piece of text is to measure the occurrence frequency of the n-grams occurring in the text. An n-gram is a sequence of n consecutive symbols of the alphabet used in the text (for example, UTF8 symbols). For instance, the most frequent trigrams in Spanish are \textit{DEL} and \textit{QUE}. On the other hand, the trigrams \textit{WHE} and \textit{OP+} are very rare in Spanish, but not in an SQL sentence, as they apear in the keywords DROP and WHERE. 

We discarded the approach of analysing word ocurrences insted of n-gram ones, because this requires a tokenization pre-processing of the text, which can be much more complicated. Tokenization is the process of spliting the text into tokens (sequences of consecutive symbols) between two delimiters, which are then discarded from the analysis. In natural language, there are very few delimiters (namely, blanks, dots, commas) and they are not relevant for identifying the language. This is not the case of programming languages, where the number of nested parentheses, semicolons or other delimiters is usually higher than in natural language and relevant for the analysis. Moreover, the discarding the semicolon delimiters in a sequence like \texttt{‘;DROP+TABLE+users;SELECT+userinfo+WHERE+‘t’=‘t} results in a loss of information, as the opening sequence \texttt{';} is a common pattern in SQL injection attacks. Furthermore, the tokenization process depends on the lexical conventions of each programming language, as the delimiters vary from one programming language to the other. An alternative could be to consider as potential delimiters the union of the delimiters of all the programming languages to consider. However, some symbols that are delimiters in one programming language may be part of programming constructions in other languages, so suppressing them may destroy relevant sequences for the analysis. On the other hand, a text can be split in n-grams following a very simple, fast and linear algorithm, in which a windows of length \textit{n} is shifted all along the text, and the process is the same independently from the programming languages to be considered.

In spite of the enumerated advantages, there is at least one case in which there exist a clear delimiter that justifies adopting a word tokenization approach rahter than n-grams: the URI field of the HTTP requests is a sequence of character strings separated by the slash symbol (\texttt{/}). For this case paticular case we adopt the usual tokenization approach, splitting the whole string into the sub-strings delimited by the slashes. Once this is done, the approach will be the same as for n-grams, just taking each sub-string as a single symbol. For example, the URI \texttt{http://www.example.com/store/members} after tokenization is the list $["\texttt{http:}", "\texttt{}", "\texttt{www.example.com}", "\texttt{store}", "\texttt{members}"]$, and a bigram of this sequences is $[\texttt{store},\texttt{members}]$.

Another requirement to be fulfilled is to be able to provide \textit{evidence} of the anomaly when an HTTP request is rejected. When using n-grams as attributes this can be easily related to some pieces of the input data that are considered outliers.

\section{N-gram model framework}

We developed and compare three different models based on n-gram frequencies. All the three models are different instances of the same pattern. We start from a set $Tr$ of training HTTP requests, that we assume are legit requests. Let us call $F=N_H \times P$ the set of \textit{model fields} formed by an HTTP request field name in $N_H$ and either a parameter name in $P$ (if the field contains parameters, as may be the case in the \texttt{URI} or \texttt{Body} fields), or a special value $\bot$, otherwise. The collection of attributes $A$ of the model is the set of n-grams contained in the contents of each model field in the training set $Tr=\{r_1,\ldots r_{\mid Tr \mid}\}$. In other words, we do not consider the raugh n-grams in the HTTP request fields, but we parse the command structure, and consider each form field in the web application as an independent language to be modeled. The variable $n$ is consider a parameter of the model that can be adjusted. In practice, we only consider $n=1,2,3$. Beyond 3, the frequency of any n-gram become very rare, and threfore meaningless for identifying the language.

We associate a random variable $X_a$ to each attribute $a=(x,z) \in A$, which measures events related to the ocurrences of the n-gram $z$ in the model field $x$. We focus on two types of events: the number of ocurrences of the n-gram in the model field (integer value) or its frequency (that is, the number of ocurrences of the n-gram divided into the total amount of n-grams of this model field). Measuring the number of n-grams is better suited for those model fields having an enumerated type of possible values. Mesuring the n-gram frequency performs better when the field length may significantly vary from one requesto to another. For example, a message field in a contact form of the web application may be arbitrarly long, so the number of ocurrences of a given symbol may be quite different depending on the message. However, the frequency of that symbol will be rather stable.

%We consider four possible types of random variables, resulting from the combination of the following criteria:
%\begin{itemize}
%\item Whether the random variable measures 
%\item Whether each HTTP requests gives raise to an independent sample of the random variable, or the whole training set $Tr$ is considered as a single sample. In the first case, the value of $X_a$ for the whole set results from the average for each HTTP request sample, while in the second this value is computed exactly from all the samples.
%\end{itemize}

A distribution for these variables is a tuple $d=(\mu, \sigma, max, min, N_d)$, where $\mu \in R$ is the distribution mean, $\sigma^2 \in R$ the distribution variance, $max$ and $min$ the maximum and minimum values that were sampled for $X$, and $N_d$ the number of sampled values. We consider two functions $singl : R \rightarrow D$ which constructs a distribution $(r,0,r,r,1)$ from a single real value $r$ and a function $sample: D \rightarrow R \rightarrow D$ which adjusts the parameters of a previous aproximation to a given distribution $d$ with a new sampled value $r$. A map $M : A \rightarrow D$ describes the expected distributions for the random variables $X_a$ of attribute $a$. This mapping provides the signature of the language associated to each model field $x$, and is inductively constructed from the training set $Tr$, iterating the $sample$ function from the initial application of the $singleton$ one.

The arriving HTTP request shall be analized on-line, one by one. In order to test an HTTP request $r$, the map $M_r$ is first constructed for the training singleton \{r\}. Then, a score $s=d_x(M,M_r)$ is computed for each model field $x$ in the HTTP request using a distance function $d$ which compares the distance between the expected distributions of n-grams provided by the model $M$ and the actual distributions $M_r$ extracted from $r$. If the distance satisfies some given criterion $C$, the request is considered acceptable, otherwise it is considered an outlier. By default, this criterion is being outside the rank defined byt he minimum and maximum values of the n-gram distribution, that is,  $C(s,s_r) = s.min \leq s_r.\mu \leq s.max$

This procedure is repeated for each model field $x$. If the contents of any of the model fields in the HTTP request $r$ is evaluated as an outlier, the whole HTTP request is processed as dangerous (fileterd, logged, derived to a honeypot application, depending on the web application security policy). The evidence for the anomaly of $r$ in field $x$ is provided by the set of n-grams $\{ z \mid  |M_x(z).\mu - M_{r,x}(z).\mu | > p^n_x \land M_x(z).\mu > m^n_x\}$ for some ceil distances $p^n_x$ and n-gram frequencies $m^n_x$ depending on the n-gram length $n$ and the model field $x$ (all of them taken as zero, by default). This is what enables the security analyst to understand why the WAF rejected the request. It is also what enables a fine tuning of the model.

\subsection{Model Tuning}

In practice, the training set $Tr$ will be a small fragment of all possible HTTP requests, so it may happen that a given attribute $a$ found in the test request $r$ is not present in any of the request of the training field $Tr$, but still a valid input. However, most of the fields contains values that are a subset of some larger language. For instance, a form field in the web application corresponding to an address in Montevideo is a particular case for a piece of text written in Spanish. Such larger language has its own language signature, which can be used as a prior. In the case a given n-gram is not be defined in $M$ for that field, we use that prior as the expected value for the model. We take as priors distributions the ones proposed in \cite{priors}. Several sites offer countings or the mean for n-grams made of letters extracted from large Spanish corpuses such as Wikipedia articles. Unfortunatelly, we do not found any of them considering n-grams containing blank spaces and punctuation symbols, nor proposing n-gram variances from one Spanish document to another. The variance of those n-grams in the prior is considered as being zero.

The contents of some parameters of the web application may be random data by its very nature. This is the case, for instance, of a peremeter containing the user's password: a password should be in principle any sequence of symbols, and all sequences should be equally likely. Furthermore, for security reasons, it would not be a good idea that the WAF stores information about the distribution of such sensible fields. In those cases, we simply ignore the parameter field in the model. The consequence of this choice is that we will not be able to detect attacks performed on those fields. A negative classification approach should be adopted for such fields, searching for those n-grams that are likely to appear in an attack, rather than positively characterising normal values. 

In practice, not all n-grams are relevant. Actually, some distinctions are more problematic than helpfull. For example, does the IP address 168.192.0.1 better characterise valid requests than the IP 168.192.0.2? Certainly not, and on the other side, counting the number of ocurrences of each particular IP address is more prone to overfitting. For this reason, we do not directly work on the n-grams themselves, but on an abstraction resulting from the following transformations: 
\begin{itemize}
\item Letters are uncapitalized
\item Accents are removed (for instance, the letter \textit{á} is transformed into \textit{a}
\item N-grams made only of numbers are collapsed into the capital letter \textit{N}
\end{itemize}

\section{Experimental results for three n-gram models}

Providing a particular set of random variables $X_a$ for the attributes, a particular distance $d$ between language signatures and particular outlierness criteria $C$ and evidence criteria $E$, we obtain an particular for tackling the code injection problem using n-grams. In the rest of this section, we describe the results with three models obtained from this general pattern for the CISC2010 dataset available at \cite{CISC2010}. This dataset provides a collection of normal and abnormal HTTP requests for a tiny web application for an on-line store. The application enables the user to fill a shopping car and then pay. The dataset is made of 36.000 valid request for the training set, another 36.000 different valid requests for testing normal behavior (NTR), and 25.000 abnormal test requests (ATR), mixing different kinds of attacks and valid requests with infrequent characters in the parameter fields (typos). Unfortunatelly, attacks and valid but infrequent requests are not distinguished in the ATR. 

The model performance on the data set is measured using the usual indicators for detection rate and false alarm rate. The detection rate corresponds to the porcentage of the ATR file that is detected training the model with the whole $Tr$ set. The false alarm rate corresponds to the percentage of the NTR file that is pointed as abnormal.

\subsection{Mahalanobis distance}

In this model, the random variable $X_a \in R$ associated to the attribute $a=(x,z)$ measures the frequency of n-gram $z$ in the contents of te model field $x$, that is, the number of occurrences of $z$ in the contents of $x$ divided into the total number of n-grams in $x$. The frequency in the model is estimated as the average of the frequencies for each request $r_i \in Tr$.

The distance between two language signatures for field $x$ the is mesured applying Mahalanobis distance, as described in \ref{MahalanobisDistance}. Let $N = \{ z \mid a=(x,z) \in F  \}$, Mahalanobis distance is defined as follows:

\begin{align*}
\label{MahalanobisDistance}
d_x(M,M_r) & = \sum_{z\in N} \frac{ | M_x(a).\mu - M_{r,x}(a).\mu |}{1+M_{r,x}(a).\sigma}\\
            & = \infty, \text{if} M_x(a) \text{is undefined} 
\end{align*}

The mapping $M(a)$ associates the parameters of the distribution that is the language signature for $x$. Mahalanobis distance compare the means of each n-gram distribution, and relates them to the variance of such n-gram in the model. In this way, the differences in those n-grams with high variance does not pound as differences with respect to n-grams which are rather constant frequency. A constant $1$ is added to the denominator for preventing a division by zero for those n-grams that have a constant frequency. For determining whether an n-gram $z$ is an outlier we use max-min. If the attribute $a$ was never seen in the training set, then $M_x(a)$ is undefined, and there is no reference distribution for the attribute. If this is the case, a special score value $\infty$ greater than any other possible scrore is assigned to the attribute, causing the field contents to be considered as an outlier.   

Table \ref{MahalanobisModelResults} shows the value of the indicators for this model, taking n-grams of length $n=1,2,3$. The best compromise is obtained for $n=2$.

\begin{table}[]
\centering
\caption{Mahalanobis model}
\label{MahalanobisModelResults}
\begin{tabular}{|l|l|l|l|}
\hline
                 & n = 1   & n = 2  & n = 3    \\
\hline
Detection Rate   &  0.965  & 0.984  & 0.992    \\
\hline
False Alarm Rate &  0.002  & 0.005  & 0.013    \\
\hline
\end{tabular}
\end{table}

Inspecting the evidence generated for the rejected requests, we notice that......

\subsection{Ranking}

This model is inspired from a usual method for detecting language mixures in the web \cite{XX}. The random variable $X_a \in R$ associated to the attribute $a=(x,z)$ measures the number of ocurrences of n-gram $z$ in the contents of model field $x$ along all the requests in the training set $Tr$. Then, the model is post-processed, and the n-grams ocurring in each field $x$ are arranged in decreasing order respect to the mean of its frequency.  The resulting ordered list of attributes $[a_a,\ldots a_n]$ is the language signature of model field $x$, and the model $M$ is the function $M_x(a_i)=\text{singleton}(i) \in D$ providing the position in that list. In order to compare the language signature for $x$ obtained from the test request $r$ to the expected language signature by adding the difference in the ranking position for each n-gram ocurring in $r$. 

\begin{align*}
\label{RankDistance}
d_x(M,M_r) & = \sum_{z\in N} \lvert M_x(a).\mu - M_{r,x}(a).\mu\rvert \\
           & = \infty, \text{if } M_x(a) \text{ is undefined} 
\end{align*}

Table \ref{RankModelResults} shows the value of the indicators for $n=1,2,3$.


\begin{table}[]
\centering
\caption{Rank model}
\label{RankModelResults}
\begin{tabular}{lllllllll}
                 & n = 1   & n = 2   & n = 3   \\
Detection Rate   & 0.969   &  0.990  & 0.992   \\
False Alarm Rate & 0.001   &  0.016  & 0.033   \\
\end{tabular}
\end{table}

\subsection{Mismatch count}

In the third model considered the score consists in just counting the number of evidence elements, that is, how many n-grams are outliers with respecto to their expected distribution. 

\begin{equation}
\label{MismatchCountDistance}
d_x(M,M_r) = \lvert \{ z \ mid \lvert M_x(z).\mu - M_{r,x}(z).\mu\rvert > p^n_x \& M_x(z).\mu > m^n_x\} \rvert 
\end{equation}

The model parameters $p^n_x$ and $m^n_x$ can be used to tune the model. Figure \ref{FrequenciesModelResults} shows the results when taking $p^1_x = 0.01$, $p^1_x = 0.05$ and $p^1_x = 0.1$ on one hand and $m^1_x=0.001$, $m^1_x=0.01$ and $m^1_x=0.003$ for all model field $x$.  

\begin{table}[]
\centering
\caption{Independent Frequencies model}
\label{FrequenciesModelResults}
\begin{tabular}{lrrr}
                 & n = 1   & n = 2   & n = 3   \\
Detection Rate   & 0.966   &  0.985  &  0.992  \\ 
False Alarm Rate & 0.001   &  0.004  &  0.012  \\ 
\end{tabular}
\end{table}

\section{Related work}

todo: el paper de cisc no considera ataques en campos de los http requests que puedan contener caracteres especiales. No va a las frecuencias de n-gramas más largos, solo de caracteres, y caracteres especiales permitidos. Encontrar un ataque en PKDD que contenga casos que no son cubiertos. 

todo: mix of models. email, the best is rank. Explicar que si no ponemos infinity cuando no encontramos un ngram entonces las ocurrencias de simbolos que no están en el alfabeto pesan poco, se camuflan en las diferencias de frecuencia habituales.


\section{Conclusions}


\subsubsection*{Acknowledgments}
...

In the bibliography, use \texttt{\textbackslash textsuperscript} for ``st'', ``nd'', ...:
E.g., \enquote{The 2\textsuperscript{nd} conference on examples}.
When you use \href{http://www.jabref.org}{JabRef}, you can use the clean up command to achieve that.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs03}
\bibliography{paper}

All links were last followed on October 5, 2014.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

    Status API Training Shop Blog About 

    © 2016 GitHub, Inc. Terms Privacy Security Contact Help 

