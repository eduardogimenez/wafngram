 \documentclass[runningheads,a4paper]{llncs}

\usepackage[american]{babel}
\usepackage[american]{amsmath}
\usepackage[american]{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage{calligra}
\usepackage[T1]{fontenc}

%better font, similar to the default springer font
%cfr-lm is preferred over lmodern. Reasoning at http://tex.stackexchange.com/a/247543/9075
\usepackage[%
rm={oldstyle=false,proportional=true},%
sf={oldstyle=false,proportional=true},%
tt={oldstyle=false,proportional=true,variable=true},%
qt=false%
]{cfr-lm}
%
%if more space is needed, exchange cfr-lm by mathptmx
%\usepackage{mathptmx}

\usepackage{graphicx}

%extended enumerate, such as \begin{compactenum}
\usepackage{paralist}

%put figures inside a text
%\usepackage{picins}
%use
%\piccaptioninside
%\piccaption{...}
%\parpic[r]{\includegraphics ...}
%Text...

%Sorts the citations in the brackets
%\usepackage{cite}

\usepackage[T1]{fontenc}

%for demonstration purposes only
\usepackage[math]{blindtext}

%for easy quotations: \enquote{text}
\usepackage{csquotes}

%enable margin kerning
\usepackage{microtype}

%tweak \url{...}
\usepackage{url}
%nicer // - solution by http://tex.stackexchange.com/a/98470/9075
\makeatletter
\def\Url@twoslashes{\mathchar`\/\@ifnextchar/{\kern-.2em}{}}
\g@addto@macro\UrlSpecials{\do\/{\Url@twoslashes}}
\makeatother
\urlstyle{same}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother

%diagonal lines in a table - http://tex.stackexchange.com/questions/17745/diagonal-lines-in-table-cell
%slashbox is not available in texlive (due to licensing) and also gives bad results. This, we use diagbox
%\usepackage{diagbox}

%required for pdfcomment later
\usepackage{xcolor}

% new packages BEFORE hyperref
% See also http://tex.stackexchange.com/questions/1863/which-packages-should-be-loaded-after-hyperref-instead-of-before

%enable hyperref without colors and without bookmarks
\usepackage[
%pdfauthor={},
%pdfsubject={},
%pdftitle={},
%pdfkeywords={},
bookmarks=false,
breaklinks=true,
colorlinks=true,
linkcolor=black,
citecolor=black,
urlcolor=black,
%pdfstartpage=19,
pdfpagelayout=SinglePage,
pdfstartview=Fit
]{hyperref}
%enables correct jumping to figures when referencing
\usepackage[all]{hypcap}

%enable nice comments
\usepackage{pdfcomment}
\newcommand{\commentontext}[2]{\colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{\pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

%compatibality with TODO package
\newcommand{\todo}[1]{\commentatside{#1}}

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link
\usepackage[capitalise,nameinlink]{cleveref}
%Nice formats for \cref
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}

\usepackage{xspace}
%\newcommand{\eg}{e.\,g.\xspace}
%\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\eg}{e.\,g.,\ }
\newcommand{\ie}{i.\,e.,\ }

%introduce \powerset - hint by http://matheplanet.com/matheplanet/nuke/html/viewtopic.php?topic=136492&post_id=997377
\DeclareFontFamily{U}{MnSymbolC}{}
\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
\DeclareFontShape{U}{MnSymbolC}{m}{n}{
    <-6>  MnSymbolC5
   <6-7>  MnSymbolC6
   <7-8>  MnSymbolC7
   <8-9>  MnSymbolC8
   <9-10> MnSymbolC9
  <10-12> MnSymbolC10
  <12->   MnSymbolC12%
}{}
\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathbb{D}}
%\newcommand{\A}{\mathbb{A}}
\newcommand{\A}{\calligra A}
\newcommand{\N}{\mathbb{N}}
\newcommand{\HTTP}{\textsc{http}\xspace}
\newcommand{\HTML}{\textsc{html}\xspace}
\newcommand{\dist}{\text{dist}}



\begin{document}
%This allows a copy'n'paste of the text from the paper
\input glyphtounicode.tex
\pdfgentounicode=1

\title{An n-gram framework for recognizing web applications attacks}
%If Title is too long, use \titlerunning
%\titlerunning{Short Title}

%Single institute
\author{Eduardo Gim\´enez}
%If there are too many authors, use \authorrunning
%\authorrunning{First Author et al.}
\institute{ICT4V}

%Multiple institutes
%Currently disabled
%
\iffalse
%Multiple institutes are typeset as follows:
\author{Firstname Lastname\inst{1} \and Firstname Lastname\inst{2} }
%If there are too many authors, use \authorrunning
%\authorrunning{First Author et al.}

\institute{
Institute 1\\
\email{...}\and
Institute 2\\
\email{...}
}
\fi
			
\maketitle

\begin{abstract}
We present a framework for detecting code injection attacks on web
applications based the n-gram frequency signature of HTTP fields and application
parameters. The approach is based on a positive description of the
expected application behavior, so it can detect new attack techniques more
easily than the rule-based approach that is available in Web Application Firewalls such as
ModSecurity. It provides a high accuracy level and low false alarms compared
with the OWASP core rule set for ModSecurity. Experimental results shows
that the approach is compatible with time constrains imposed for on-line package
analysis.
\end{abstract}

\keywords{Web Application Firewall, anomaly detection, n-gram model}

By its very nature, web applications are designed to be exposed in the Internet. This means that its owner can not govern who actually connects to the application and  therefore are available to any Internet user. This feature makes them a primary target of any attacker that wants to get access to the assets (data bases) that the application manages or to place baits (e.g., fake URLs to his own site) to lure honest users.

The code of a web application frequently contains vulnerabilities like the ones listed in \cite{OWASPTop10}. Searching and fixing these vulnerabilities in the application code is a time consuming activity, and frequently the application can not remain off-line until they have been fixed. For this reason, we are rather interested in virtual patching, a technique in which a second software component, external to the web application, is placed between the web application and its users. This component, usually called a Web Application Firewall (WAF), intercepts and inspects all the traffic between the server and the clients, searching for attacks not in the headers but inside the \HTTP packet contents. Once recognized, the suspicious packets are then processed in a different way (logged, suppressed, derived to a honeypot application, etc.). ModSecurity \cite{ModSecurity} is an open source, widely used WAF enabling real-time web application monitoring, logging, and access control. 

The actions that ModSecurity undertakes are driven by rules. The administrator specifies rules about the contents of the \HTTP packets through regular expressions. ModSecurity intercepts each packet from and to the protected web application. If the packet matches a rule, then the actions specified in that rules are fired. ModSecurity offers a default package of rules for tackling the most usual vulnerabilities spotted in the \cite{OWASPTop10}, known as the OWASP ModSecurity Core Rule Set (CRS). However, an approach only based on rules also has some drawbacks: rules are a static and rigid by nature and the CRS usually produce a rather high rate of false positives \cite{ModSecurityFalseRate}. Rule tuning is a time consuming and error prone task, which has to be manually performed for each specific web application. Our long term purpose is to improve ModSecurity with anomally detection techniques that provide higher levels of flexibility and adaptability. Those approaches take advantage of sample data about what the normal behavior of the web application is, in order to spot suspicious situations which fall out of this nominal use (anomalies), and which could correspond to on-going attacks. 

\section{Code injection attacks from a language perspective}

In a usual use case of a web application, the user has to fill a form with data, which is then used to assemble an SQL database query and return the retrieved information to the user. In many programming languages, the programmer assembles the query concatenating strings, including part of the data supplied by the user in the form. If such data is not correctly sanitized, an attacker may take advantage of this mthod to inject a carefully crafted piece of code that discloses unauthorized information or internally damages the application. For example, let us assume that the application displays the form field \textit{user} and assembles the following query from its contents \textit{query := \enquote{\textsc{select} * \textsc{from} users \textsc{where} name = '} + userName \textsc{+} \enquote{'} }, where the plus stands for string concatenation. If the value \enquote{Juan Perez} is supplyed as user name, then Juan Perez's record in the database is returned and displayed. However, should the attacker supply the string \textit{Juan Perez' \textsc{or nor like} 'xx}, the assembled query becomes \textit{\enquote{\textsc{select} * \textsc{from} users \textsc{where} name = ‘Juan Perez’ \textsc{or not like} 'xxx'}}, which returns Juan Perez's record plus all the records of users whose name does not match the string \textit{xxx} (which are probably all the users in the database). 

In its most general version, code injection consists in the malicious provision of unexpected input data which modifies the intended purpose of an executable, interpreted piece of code. Beyond SQL injection, this technique may be applied to several query languages that are usually used in web applications, such as \textsc{xldap}, \textsc{os} commands, \textsc{xpath}, \textsc{xml} parsers, \textsc{SMTP} headers, \textsc{nosql}, among others. It can be applied not only to human supplied form fields, but also to other fields in the \HTTP requests that usually are automatically filled by the web navigator, and which are used on the web server side to perform queries about supported features, better rendering, etc. In the sequel, we use the term \textit{field} to mean either an application web form field, or one of the fields described in the \HTTP protocol.

From a language perspective, code injection attacks always involve mixing two languages: the expected language for a given field and a piece of a query programming language. In the former example, the expected language for the field correspond to personal names in Spanish, while the query programming language is SQL. Starting from this fact, detecting such kind of attacks may be considered as a particular case of detecting multiple languages in a given piece of text. Our research program is therefore to find conexions between NLP techniques and WAF attack detection, through the application of pattern recognition approaches.

A well known technique for identifying the language of a piece of text is to measure the frequency of the n-grams occurring in the text. An n-gram is a sequence of $n$ consecutive symbols of the alphabet used in the text (for example, \textsc{utf8} symbols). For instance, the most frequent trigrams in Spanish are \textit{del} and \textit{que}. On the other hand, the trigrams \textit{whe} and \textit{ike} are very rare in Spanish, but not in an SQL sentence, as they apear in the keywords \textsc{where} and \textsc{like}. 

A string can be split in n-grams following a very simple, fast and linear algorithm, in which a windows of length \textit{n} is shifted all along the text, and the process is the same independently from the programming languages to be considered. In the case of the URI field there is a clear word delimiter (the slash symbol: /), so the string is separated into a list of words (not characters) separated by this delimiter. Once this is done, the approach is the same as for n-grams, just taking each word as a single symbol of the URI language. 

\section{N-gram model framework}

We developed and compare three different models based on n-gram frequencies, all of them instances of the same framework. The variable $n$ is consider a parameter of the model that can be adjusted. In practice, we only consider n-grams of length $m$, with $m \leq n=1,2,3$. Beyond 3, the frequency of any n-gram become very rare, and threfore meaningless for identifying the language.

We start from a training set $Tr=\{r_1,\ldots r_{\mid Tr \mid}\}$ of legit \HTTP requests. An \HTTP request is just a string, as illustrated in Figure \ref{\HTTPRequest}. It is made of a header, a collection of field names and their values separated by a semicolon, and a request body. The \HTTP protocol does not impose any structure on a request body. However, most of the interaction with the web application user consists in presenting \HTML forms to be filled and answers to the filled forms. Once filled, \HTML forms are transmitted to the server as a list of pairs $\text{m}_1=\text{v}_1 \text{&} \ldots \text{m}_k=\text{v}_k$ separated by the \textit{&} symbol. We do not work directly form the request string itself. We first parse its structure, keeping the \HTTP field and parameter contents, and exclluding their names from the n-gram analysis. Therefore, the \textit{model fields} are formed by an \HTTP request field name in $N_H$ and either a parameter name from a $P$, or a special value $\bot$, otherwise. The collection of attributes $A$ of the model is the set of n-grams extracted from the contents of each model field in the requests of $Tr$.

The \textit{attributes} of the model are the n-grams that can be found in the model fields contents. Each attribute is a pair $(x,z) \in A$ made from a model field $x$ and an n-gram $z$. We associate a random variable $X_a$ to each attribute $a=(x,z) \in A$, which measures events related to the ocurrences of the n-gram $z$ in the model field $x$. We focus on two types of events: the number of ocurrences of the n-gram in the model field (integer value) or its frequency (that is, the number of ocurrences of the n-gram divided into the total amount of n-grams of this model field). Measuring the number of n-grams is better suited for those model fields having an enumerated type of possible values. Mesuring the n-gram frequency performs better when the field length may significantly vary from one requesto to another. For example, a message field in a contact form of the web application may be arbitrarly long, so the number of ocurrences of a given symbol may be quite different depending on the message. However, the frequency of that symbol will be rahter the same in all messages.

%We consider four possible types of random variables, resulting from the combination of the following criteria:
%\begin{itemize}
%\item Whether the random variable measures 
%\item Whether each \HTTP requests gives raise to an independent sample of the random variable, or the whole training set $Tr$ is considered as a single sample. In the first case, the value of $X_a$ for the whole set results from the average for each \HTTP request sample, while in the second this value is computed exactly from all the samples.
%\end{itemize}

A \textit{distribution} for these variables is a tuple $d \in \D = (\mu, \sigma, max, min, N_d)$, where $\mu \in \R$ is the distribution mean, $\sigma^2 \in \R$ the distribution variance, $max,min \in \N$ the maximum and minimum values that were sampled for $X$, and $N_d\in\N$ the number of sampled values. A distribution is iteratively constructed using the on-line computation of its parameters described in \cite{Knuth}. This iterative process uses two functions: $\text{singleton} : \R \rightarrow D$ which constructs the single-point distribution $(r,0,r,r,1)$ from a single real value $r$, and $\text{sample}: \D \rightarrow \R \rightarrow \D$ which adjusts the parameters of a previous aproximation to a given distribution $d$ with a new sampled value $r$ as it is explained in \cite{Knuth}. A map $M : \A \rightarrow \D$ describes the expected distributions for the random variables $X_a$ of attribute $a$. This mapping provides the signature of the language associated to each model field $x$, constructed iterating these two functions from the training set $Tr$.

The WAF shall analyze the \HTTP requests on-line, one by one. In order to test a given request $r$, the map $M_r$ for the training singleton $\{r\}$ is first constructed. Then, each \HTTP field $x$ of $r$ is considered. If $x$ is not defined in the mapping $M$, then $r$ is rejected. Otherwise, a score $s=\dist_x(M,M_r)$ is computed for $x$ using a distance function $\textit{dist}_x$ which compares the distance between the expected distributions of n-grams provided by the model $M$ for $x$ and the actual distributions $M_r$ extracted from the field $x$ of $r$. Different distances may be associated to differente fields. If the score satisfies some given criterion $C$, the request is considered valid, otherwise it is considered an outlier. The default criterion is being outside the rank defined by the minimum and maximum values of the n-gram distribution, that is,  $C(s,s_r) = s.min \leq s_r.\mu \leq s.max$.

If theres is a model field $x$ of $r$ for which its ontent is evaluated as an outlier, then the whole \HTTP request is deemed as anomalous and processed accodingly. The evidence for the anomaly of $r$ in field $x$ is provided by the set of n-grams $\{ z  \mid  \lvert M(x,z).\mu - M_{r}(x,z).\mu \rvert > p^n_x \land M_x(z).\mu > m^n_x\}$ for some given ceil values $p^n_x$ and n-gram frequencies $m^n_x$, which are parameters of the model. This evidence is what enables the security analyst to understand why the WAF rejected the request. It is also what enables a fine tuning of the model. Notice that these parameters depend on the n-gram length $n$ and the model field $x$. By default these parameters are taken as zero. 

In practice, the training set $Tr$ will be a small fragment of all possible \HTTP requests, so it may happen that a given attribute $a$ found in the test request $r$ is not present in $Tr$, but still a valid input. However, most of the fields contains values that are a subset of some larger language. For instance, a form field in the web application corresponding to an address in Montevideo is a particular case for a piece of text written in Spanish. Such larger language has its own language signature, which can be used as a prior. Should a given n-gram not be defined in $M$ for that field, we use that prior as the expected value for the model. 

Some parameters of the web application may contain random data by its very nature. For instance, a parameter containing the user password should be in principle any sequence of symbols, and all sequences should be equally likely. In such cases there is no hope to find a language signature for the field, so the field is excluded from the model. In other cases, even if the language is not completely random, the principal feature regards the allowed symbols, but not how they are combiined. For example, a parameter containing e-mail addresses contains at most one single \texttt{@} symbol, frequently a single dot in the domain name, no other special characters, but almost any possible combination of two letters can be expected. As a consequence, a particular $n$ is chosen for each field. 

In practice, not all n-grams are relevant. Actually, some distinctions are more problematic than helpfull. For example, the IP address 168.192.0.1 does not better characterise valid requests than the IP 168.192.0.2. On the other side, counting the number of ocurrences of each particular IP address is more prone to overfitting. For this reason, we do not directly work on the n-grams themselves, but on an abstraction resulting from the following transformations: (a) Letters are uncapitalized, (b) accents are removed and (c) n-grams made only of numbers are collapsed into the capital letter \enquote{\textit{N}}.

\section{Three n-gram models}

Providing a particular set of random variables $X_a$ for the attributes, a particular distance $d$ between language signatures and particular outlierness criteria $C$ and evidence criteria $E$, we obtain an particular for tackling the code injection problem using n-grams. 

%\subsection{Mahalanobis distance}

\textbf{Mahalanobis distance.} In this model, the random variable $X_a \in R$ associated to attribute $a=(x,z)$ measures the number of occurrences of $z$ in the contents of $x$ divided into the total number of n-grams in $x$, i.e., the frequency of $z$ in $x$. The distance between two language signatures is mesured using Mahalanobis distance \ref{MahalanobisDistance}. Let $N_x = \{ z \in S^* \mid a=(x,z) \in \A  \}$ the set of n-grams collected for field $x$. Mahalanobis distance is defined as follows:
\begin{align*}
\label{MahalanobisDistance}
\textit{dist}_x(M,M_r) & = \sum_{z\in N} \frac{ | M_x(a).\mu - M_{r,x}(a).\mu |}{1+M_{r,x}(a).\sigma}\\
           & = \infty, \text{if } M_x(a) \text{ is undefined} 
\end{align*}
Mahalanobis distance compare the means of each n-gram distribution, and relates them to the variance of such n-gram. In this way, differences for n-grams with high frequency variance do not weight as differences with respect to n-grams with a rather constant frequency. A constant $1$ is added to the denominator to prevent a division by zero for those n-grams with constant frequency. If the n-gram $z$ was never seen in the training set nor in the prior, then $M_x(a)$ is undefined, a special score value $\infty$ greater than any other possible scrore is assigned to the attribute, causing the field contents to be considered as an outlier.   

\textbf{Ranking.} This model is inspired from a usual method for detecting multilanguage pages in the web \cite{Cavnar94n-gram-basedtext}. The random variable $X_a \in R$ associated to the attribute $a=(x,z)$ measures the number of ocurrences of n-gram $z$ in the contents of model field $x$ along all the requests in the training set $Tr$. Then, the model is post-processed, and the n-grams ocurring in each field $x$ are arranged in decreasing order respect to the mean of its frequency.  The resulting ordered list of attributes $[a_a,\ldots a_n]$ is the language signature of model field $x$, and the model $M$ is the function $M_x(a_i)=\text{singleton}(i) \in D$ providing the position in that list. In order to compare the language signature for $x$ obtained from the test request $r$ to the expected language signature by adding the difference in the ranking position for each n-gram ocurring in $r$. 
\begin{align*}
\label{RankDistance}
d_x(M,M_r) & = \sum_{z\in N} \lvert M_x(a).\mu - M_{r,x}(a).\mu\rvert \\
          & = \infty, \text{if } M_x(a) \text{ is undefined and} \lvert a\rvert = 1 \\
          & = \lvert  N_x \rvert, \text{if } M_x(a) \text{ is undefined and} \lvert a\rvert > 1
\end{align*}
Monograms that were not seen for this field in the training dataset nor in the prior distribution associated to the model field (if any) take the $\infty$ score and are therefore rejected. The reason is that unexpected symbols are usually a good indicator of a code injection attack. On the other side, the possibility of missing valid bigrams and trigrams in the training set is considerable, so those n-grams do not cause an inmediate rejection. Instead, they contribute with a high score, equal to the length of the list of seen n-grams for this field. 

\textbf{Mismatch count.} In this model the score consists in just counting the number of evidence elements, that is, how many n-grams are outliers with respecto to their expected distribution. 
\begin{equation}
\label{MismatchCountDistance}
d_x(M,M_r) = \lvert \{ z \ mid \lvert M_x(z).\mu - M_{r,x}(z).\mu\rvert > p^n_x \& M_x(z).\mu > m^n_x\} \rvert 
\end{equation}
The parameters $p^n_x$ and $m^n_x$ can be used to tune the model. Parameter $p^n_x$ is used to prune those n-grams that are very 

\section{Experimental results}

The described framework was implemented using the Ocaml programming language \cite{wafngram}. We tested the three n-gram models proposed on the CISC2010 dataset \cite{CISC2010Dataset}. This dataset provides a collection of normal and abnormal \HTTP requests for a tiny web application for an on-line store. The application enables the user to fill a shopping car and then pay. The dataset is made of 36.000 valid request for the training set, another 36.000 different valid requests for testing normal behavior (NTR), and 25.000 abnormal test requests (ATR), mixing different kinds of attacks and valid requests with infrequent characters in the parameter fields (typos).

The model performance on the data set is measured using the usual indicators for detection rate and false alarm rate. The detection rate corresponds to the porcentage of the ATR file that is detected training the model with the whole training dataset. The false alarm rate corresponds to the percentage of the NTR file that is pointed as abnormal. Table \ref{ExperimentalResults} shows the indicators obtained for each type of model and given n-gram length. In the case of the mismatch count model, the table shows the results when taking $p^1_x = 0.01$, $p^1_x = 0.05$ and $p^1_x = 0.1$ on one hand and $m^1_x=0.001$, $m^1_x=0.01$ and $m^1_x=0.003$ for all model field $x$. 
 
\begin{table}[]
\centering
\label{ExperimentalResults}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
& \multicolumn{3}{c}{\textbf{Mahalanobis}} \vline & \multicolumn{3}{c}{\textbf{Rank}}\vline  & \multicolumn{3}{c}{\textbf{Mismatch count}} \vline\\
\hline
\hline
       Ngram length       &  1   &    2   &   3   &   1   &   2   &   3   &    1  &    2  &    3   \\    
\hline
       Detection Rate    & 0.965 & 0.984 & 0.992 & 0.969 & 0.990 & 0.992 & 0.966 & 0.985 &  0.992  \\  %0.965 0.988   0.979
\hline
       False Alarm Rate  & 0.002 & 0.005 & 0.013 & 0.001 & 0.016 & 0.033 & 0.001 & 0.004 &  0.012  \\  %0.001 0.007   0.001
\hline
\end{tabular}
\caption{Experimental results}
\end{table}
All distances provide results in the same order of magnitude. In all cases, the best compromise between both indicators is obtained using bigrams. The best results are obtained for the mismatch count technique. 


\section{Related work}

In \cite{wang2004anomalous}, Wang and Stolfo introduced PAYL, a payload-based anomaly detector for intrusion detection. Their analyser computes the byte frequency distribution of network packets from a training dataset, and compares them to test packets using Mahalanobis distance. The objective was not to detect code injection attacks, but rather worms or other forms of malware propagating on the network. Their work report on excelent results on the 1999 DAPRA IDS dataset. In \cite{wang2006anagram}, they introduce Anagram, which extends PAYL to higher n-grams, with $n>1$. One of the main differences with respect to our work is that Wang and Stolfo's tools are oriented to network traffic, and therefore works on TPC packets, that is, they work on the network OSI layer. On the contrary, we are focused on protecting web applications, so we perform the analysis at OSI's application layer, on the \HTTP packets. Sticking to the application layer enables us to develop a dedicated model for each \HTTP field and application parameter, something that cannot be done at the TCP layer. 

Torrano, Perez and Marañón from the CISC team developed a model which associates the frequencies of of letters, digits and special characters to each application parameter \cite{torrano2010anomaly}. The tool they mixes filter rules and statistical checkings. Filter rules specify the only headers, directories and \HTTP field values that are permitted. Only the web application parameters are subject to statistical checks with respect to the aforementioned attributes. On the contrary, our approach is fully statistical, is based on a less radical abstraction of the field contents and makes use of higer order n-grams. Moreover, it can distinguish attacks that cannot be detected as anomalous cases when all symbols are collapsed in those three categories. Consider for instance a form field for names, and let us assume that there is at least one name in the training dataset containing the left single quotation mark (like \textit{Ed++O'Neil}). The code injection sample \textit{'+\textsc{OR+NOT+LIKE}+'xxxxxxx} results in a SQL query returning all the names in the application database. However, both strings are made of the same symbols and the frequency of spaces and quotation marks is exactly the same, while the number of letters is close enough to fall inside the minimum and maximum occurrences for the field in the training set. The code injection sample has great chances of being considered a valid name according to the model in \cite{torrano2010anomaly}, but is rejected in the n-gram framework we proposed.

\todo{Vigna et all}

Other statistical approaches to the detection of web application attacks have been also proposed, for example in \cite{Gallagher}. Those works focus on a negative approach, trying to correlate the ocurrences of some attributes with the code used in the attacks that in the current state of the art. The main difference of our work with respect to them is that we focus on characterising the normal behavior of the web application. This make our approach more plastic and resiliant to variants of known attacks or even to completely new ones. 

%\section{Conclusions}

%A further line of work is to extent this anomaly detection technique to attack classification. This could be achieved by training and storing the signature of different query programming languages, such as SQL, XPath, Unix OS commands, and comparing the anomalous piece of content against those language signatures. To step forward in this direction, a problem to be solved is the generation of training data sets for such languages, necessary to construct their signatures. Such data sets have to be large enough and representative of the state of the art in injection attacks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs03}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

    Status API Training Shop Blog About 

    © 2016 GitHub, Inc. Terms Privacy Security Contact Help 

